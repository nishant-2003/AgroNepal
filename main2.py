

import os
from dotenv import load_dotenv
from typing import List, TypedDict

from langchain_community.document_loaders import TextLoader
# --- MODIFIED IMPORT ---
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults

from langgraph.graph import StateGraph, END

# --- 1. Setup Environment ---
# Load environment variables from a .env file
load_dotenv()

# Ensure your API keys are set
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OpenAI API key not found. Please set it in your .env file.")
if "TAVILY_API_KEY" not in os.environ:
    raise ValueError("Tavily API key not found. Please set it in your .env file.")

# --- 2. Define the State for the Graph ---
# This dictionary will be passed between nodes, carrying the data.
class GraphState(TypedDict):
    """
    Represents the state of our RAG graph.

    Attributes:
        query: The question asked by the user.
        documents: A list of documents retrieved from the vector store or web search.
        generation: The final answer generated by the LLM.
        decision: The decision made by the document grader ('generate' or 'rewrite').
    """
    query: str
    documents: List[str]
    generation: str
    # --- ADDED FOR TYPE SAFETY ---
    decision: str



# --- 3. Set up a Local Retriever with ChromaDB ---
# For this example, we'll create an in-memory ChromaDB vector store.
# In a real application, you could persist it by providing a directory path.
# NOTE: Make sure you have chromadb installed: pip install chromadb
documents_text = """
LangGraph is a library for building stateful, multi-actor applications with LLMs.
It extends the LangChain Expression Language with the ability to coordinate multiple
chains (or actors) across multiple steps of computation in a cyclic manner.
The main use cases for LangGraph are for adding cycles and managing agent state
in your LLM applications.
"""
with open("sample_doc.txt", "w") as f:
    f.write(documents_text)

loader = TextLoader("./sample_doc.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()

# --- MODIFIED VECTOR STORE ---
# Use Chroma instead of FAISS. It works in-memory by default.
# For persistence, you could add: persist_directory="./chroma_db"
vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)
retriever = vectorstore.as_retriever()

# --- 4. Set up the Web Search Tool ---
web_search_tool = TavilySearchResults(k=3)

# --- 5. Initialize the LLM ---
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --- 6. Define the Nodes of the Graph ---

def retrieve_documents(state: GraphState):
    """
    Retrieves documents from the local vector store.
    """
    print("---RETRIEVING DOCUMENTS---")
    query = state["query"]
    # The retriever interface is the same for Chroma and FAISS
    documents = retriever.invoke(query)
    # Convert Document objects to string content
    doc_strings = [doc.page_content for doc in documents]
    return {"documents": doc_strings, "query": query}

def grade_documents(state: GraphState):
    """
    Grades the relevance of the retrieved documents to the query.
    This node's output will be used by the router to decide the next step.
    """
    print("---CHECKING DOCUMENT RELEVANCE---")
    query = state["query"]
    documents = state["documents"]

    if not documents:
          print("---DECISION: NO DOCUMENTS FOUND, ROUTING TO WEB SEARCH---")
          # The key 'decision' is added to the state here
          return {"decision": "rewrite"}

    prompt = PromptTemplate(
        template="""
        Analyze the following user query and the retrieved documents.
        Decide if the documents are directly relevant to answer the query.
        Respond with only 'yes' or 'no'.

        Query: "{query}"

        Documents:
        {documents}
        """,
        input_variables=["query", "documents"],
    )

    chain = prompt | llm | StrOutputParser()
    decision = chain.invoke({"query": query, "documents": "\n\n".join(documents)})

    if "yes" in decision.lower():
        print("---DECISION: DOCUMENTS ARE RELEVANT, ROUTING TO GENERATE---")
        return {"decision": "generate"}
    else:
        print("---DECISION: DOCUMENTS NOT RELEVANT, ROUTING TO WEB SEARCH---")
        return {"decision": "rewrite"}

def transform_query(state: GraphState):
    """
    If documents are not relevant, this node rephrases the query for a web search.
    """
    print("---TRANSFORMING QUERY FOR WEB SEARCH---")
    query = state["query"]
    prompt = PromptTemplate(
        template="""You are a query expansion expert. Your task is to rewrite the user's question
        to be more effective for a web search. Keep the core meaning but make it more concise
        and keyword-focused.

        Original Query: {query}
        Rewritten Query:""",
        input_variables=["query"],
    )
    chain = prompt | llm | StrOutputParser()
    better_query = chain.invoke({"query": query})
    print(f"New query: {better_query}")
    return {"query": better_query}

def web_search(state: GraphState):
    """
    Performs a web search using the Tavily tool.
    """
    print("---PERFORMING WEB SEARCH---")
    query = state["query"]
    search_results = web_search_tool.invoke({"query": query})
    # We need to format the search results into a list of strings
    documents = [f"Title: {res['title']}\nSnippet: {res['content']}" for res in search_results]
    return {"documents": documents}

def generate_answer(state: GraphState):
    """
    Generates the final answer using the retrieved context.
    """
    print("---GENERATING FINAL ANSWER---")
    query = state["query"]
    documents = state["documents"]
    prompt = PromptTemplate(
        template="""You are an assistant for question-answering tasks.
        Use the following pieces of retrieved context to answer the question.
        If you don't know the answer, just say that you don't know.
        Be concise.

        Question: {question}

        Context:
        {context}

        Answer:""",
        input_variables=["question", "context"],
    )

    chain = prompt | llm | StrOutputParser()
    generation = chain.invoke({"context": "\n\n".join(documents), "question": query})
    return {"generation": generation}


# --- 7. Define the Router (Conditional Edge) ---
def route_decision(state: GraphState):
    """
    The routing logic that directs the graph's flow based on document relevance.
    """
    # The 'decision' key is now expected in the state after the 'grade' node runs
    decision = state["decision"]
    if decision == "generate":
        return "generate"
    elif decision == "rewrite":
        return "transform_query"

# --- 8. Build and Compile the Graph ---
workflow = StateGraph(GraphState)

# Add nodes
workflow.add_node("retrieve", retrieve_documents)
workflow.add_node("grade", grade_documents)
workflow.add_node("transform_query", transform_query)
workflow.add_node("web_search", web_search)
workflow.add_node("generate", generate_answer)

# Build graph
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade")
workflow.add_conditional_edges(
    "grade",
    route_decision,
    {
        "generate": "generate",
        "transform_query": "transform_query",
    },
)
workflow.add_edge("transform_query", "web_search")
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

# Compile the graph
app = workflow.compile()

# --- 9. Visualize and Run ---

png_data=app.get_graph().draw_mermaid_png()
output_filename="graph_visualization2.png"

with open(output_filename,"wb") as f:
    f.write(png_data)


# Example 1: A query that can be answered by the local documents
print("\n" + "="*50)
print("--- RUNNING EXAMPLE 1 (LOCAL RETRIEVAL) ---")
inputs_local = {"query": "What are the main use cases for LangGraph?"}
for output in app.stream(inputs_local):
    for key, value in output.items():
        print(f"Finished node '{key}':")
print("\n--- FINAL RESULT (LOCAL) ---")
print(value["generation"])
print("="*50 + "\n")


# Example 2: A query that requires a web search
print("\n" + "="*50)
print("--- RUNNING EXAMPLE 2 (WEB SEARCH) ---")
# Let's get the current time to ask a more dynamic question
from datetime import datetime
current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
inputs_web = {"query": f"What is the latest news about AI as of {current_time}?"}
for output in app.stream(inputs_web):
    for key, value in output.items():
        print(f"Finished node '{key}':")
print("\n--- FINAL RESULT (WEB) ---")
print(value["generation"])
print("="*50)
